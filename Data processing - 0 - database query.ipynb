{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This takes data from the SQL database, and saves it in a condensed trajectory form\n",
    "* This file is included for completeness, but it is not needed to reproduce the results in the paper, and also it will not run (needs database access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import pickle\n",
    "import glob\n",
    "import gzip\n",
    "import time\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sys\n",
    "import multiprocessing\n",
    "import psycopg2\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sharedcodes/bees/code\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2019"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2019\n",
    "%cd '/sharedcodes/bees/code/'\n",
    "\n",
    "if year==2018:\n",
    "    import definitions_2018 as bd\n",
    "    resultsdir = '/data/beeresults/'\n",
    "    comb_contents_dir = '/data/comb-contents-images/'\n",
    "elif year==2019:\n",
    "    import definitions_2019 as bd\n",
    "    resultsdir = '/data/beeresults2019/'\n",
    "    comb_contents_dir = '/data/comb-contents-images2019/'\n",
    "    \n",
    "import displayfunctions as bp  # 'bee plots'\n",
    "import datafunctions as dfunc\n",
    "dfunc.init(bd) \n",
    "bp.init(bd)\n",
    "bd.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One day is too long to grab at once.  Better to do one hour an iterate through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days lived - import this, because will use to only save data for bees that were alive on a given day\n",
    "# This is processed in '2- Create Data Matrix', separately for each year\n",
    "[cohort_daysalive] = pickle.load(open(resultsdir+'daysalive.pkl','rb'))\n",
    "uid_daysalive = np.concatenate(cohort_daysalive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbquery(bee_ids,day_input,bee_id_confidence_threshold=0.3,starttime = \"00:00:00.000000+00:00\",endtime = \"23:59:59.999999+00:00\",limit=44236800):  # this limit is all bees, tracked for one hour    \n",
    "    # bee_ids:  list of tag ids\n",
    "    day =  pd.Timestamp(day_input,freq='D')\n",
    "    if len(bee_ids)==0:  # none are in the list, so select none\n",
    "        bee_id_string = \" bee_id IN (-1) AND \"\n",
    "    elif len(bee_ids)==4096:\n",
    "        bee_id_string = ''\n",
    "    elif len(bee_ids)==1:\n",
    "        bee_id_string = \" bee_id = \"+str(bee_ids[0])+\" AND \"\n",
    "    else:\n",
    "        bee_id_string = \" bee_id IN \"+str(tuple(bee_ids))+\" AND \"\n",
    "    \n",
    "    if starttime<endtime:\n",
    "        daystringstart = day.strftime('%Y-%m-%d')\n",
    "        daystringend = daystringstart\n",
    "    else:\n",
    "        daystringstart = day.strftime('%Y-%m-%d')\n",
    "        day2 = day + day.freq  #  this is what the 'proper' way to do it is, because freq = 1 Day\n",
    "        daystringend = day2.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    conn = psycopg2.connect(bd.querydata)\n",
    "    df = pd.read_sql(\"SELECT * FROM \"+bd.databasename+\" WHERE \" \n",
    "                 +bee_id_string\n",
    "                     +\"bee_id_confidence>\"+str(bee_id_confidence_threshold) \n",
    "                 +\" AND timestamp BETWEEN '\" +daystringstart+\" \"+starttime+ \"' AND '\"+daystringend+\" \"+endtime+ \"' \"\n",
    "                 +\"ORDER BY timestamp \" + \n",
    "                \"LIMIT \"+str(limit), \n",
    "                 conn, coerce_float=False) \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Save condensed form of trajectory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numtimedivs=24 # how much to get from the database at once\n",
    "saveskip=1  # e.g 6 = every 2 sec\n",
    "\n",
    "if numtimedivs==24:\n",
    "    prefix = 'dayhour'\n",
    "elif numtimedivs==288:\n",
    "    prefix = 'day5min'\n",
    "else:\n",
    "    prefix = 'day1min'\n",
    "\n",
    "# I made these stricter after the first time I ran with the data.  I didn't systematically test/analyze though\n",
    "conf_threshold = 0.8\n",
    "\n",
    "min_num_obs_hour=10  # min number of obs in an hour to calculate quantities for that hour.  If don't have any data points, then the calculations don't make sense.\n",
    "\n",
    "tagids = np.arange(bd.numbees)  # this is tag id = tid, NOT unique id (uid)\n",
    "\n",
    "framesperhour = int(3*60*60)\n",
    "\n",
    "\n",
    "def processday_dftraj(daynum):\n",
    "    day = bd.alldaytimestamps[daynum]\n",
    "    print('day: ',day)    \n",
    "    \n",
    "#     comb = dfunc.day_comb_data(comb_contents_dir,daynum)\n",
    "    comb = pickle.load(gzip.open(comb_contents_dir+'comb_'+str(daynum).zfill(3 if year==2019 else 2)+'.pklz','rb'))\n",
    "\n",
    "    # Initialize an array to save movement data for bees for this day\n",
    "    # median speed, median abs value angular velocity, time active\n",
    "    # these will keep a values of -1 if there isn't enough data to do calculations\n",
    "\n",
    "    dfblank = pd.DataFrame([],columns=['daynum','framenum','uid','x','y','camera'])\n",
    "    \n",
    "    # get the bees that were alive this day.  Note, need to convert to uids, instead of tagids\n",
    "    age = np.concatenate(dfunc.getages(daynum))  # age per uid\n",
    "    uids_this_day = dfunc.convert_tagids_to_uids(np.arange(bd.numbees),daynum)  # this returns the first one, sorted by age\n",
    "\n",
    "    # don't use dayobservesel for this - therefore can put above instead of below loop\n",
    "    alive_sel = (age>=0) & (age<=uid_daysalive)  #& dayobservedsel_byuid\n",
    "    uids_alive = np.where(alive_sel)[0]\n",
    "    # make a mask, because its a more efficient way of selecting from huge arrays\n",
    "    tags_alive = dfunc.sel_cohort_bee(np.arange(bd.numbees))[alive_sel]  # note: only do this for a single day\n",
    "    tags_alive_mask = np.tile(False,bd.numbees)\n",
    "    tags_alive_mask[tags_alive] = True        \n",
    "    \n",
    "    # loop over time divisions\n",
    "    dftemps = [dfblank]\n",
    "    for timenum in range(numtimedivs):     \n",
    "        print(timenum)\n",
    "        minperdiv = np.floor(24*60 / numtimedivs).astype(int)\n",
    "        hour0, minute0 = np.divmod(minperdiv*timenum,60)\n",
    "        hour1, minute1 = np.divmod(minperdiv*(timenum+1),60)\n",
    "        starttime = str(hour0).zfill(2)+':'+str(minute0).zfill(2)+':00.000000'\n",
    "        endtime = str(hour1).zfill(2)+':'+str(minute1).zfill(2)+':00.000000'             \n",
    "        \n",
    "        # query all\n",
    "        df = dbquery(tagids,day,starttime=starttime,endtime=endtime,limit=12052340*4)\n",
    "        # always use df_to_coords, because it filters duplicates, and also applys conversion factor to x and y for 2019\n",
    "        # 21 Jan 2022 Update: df_to_coords now calculates integer framenum, and filters by this. This corrects camera-switching false id errors\n",
    "        all_camera,all_x,all_y,all_theta,all_ids,all_times,all_framenums,all_conf = dfunc.df_to_coords(df,conf_threshold=conf_threshold)        \n",
    "        # Note:  could change the code below to use all_frames to count interactions.  BUT, they are not sorted by time, because of the different cameras.  So, would have to change the whole selection process, and it doesn't really matter, so just keep it\n",
    "        \n",
    "        # loop over bees:  Get quantities per-hour\n",
    "        for beenum, bee_uid in zip(tags_alive,uids_alive):\n",
    "            idsel = (all_ids == beenum)\n",
    "            numobs = np.sum(idsel)\n",
    "            if numobs>=min_num_obs_hour:\n",
    "                camera,x,y,theta,ids,times,framenums = all_camera[idsel],all_x[idsel],all_y[idsel],all_theta[idsel],all_ids[idsel],all_times[idsel],all_framenums[idsel]\n",
    "\n",
    "                ### Calculate speed and filter by max speed.  This removes \"jumps\", or huge speed values, that are definitely tracking errors\n",
    "                # Michael said max speed from manual tracking were:  Max speed: 0.905±0.510 cm s−1 in one set of colonies, 1.071±0.438 cm s−1 in another set of colonies.  So, this value is conservative (high), to err on the side of keeping more data points, but probably does not matter\n",
    "                # Using max speed of 3 cm/sec, which is higher than any ever should have\n",
    "                max_speed = 3 * bd.pixels_per_bin   \n",
    "                # need to do an iterative procedure to remove error frames regarding speed, because the difference between points could still be very large after a single point is removed\n",
    "                numerrorframes = 1\n",
    "                while (numobs>=min_num_obs_hour) & (numerrorframes>0):\n",
    "                    dtimes_temp = np.diff(times).astype(float) * 10**-9\n",
    "                    speed_temp = dfunc.getflatdistance(x,y,camera) / dtimes_temp\n",
    "                    samecamera_temp = (np.diff(camera) == 0)\n",
    "                    error_frames = (speed_temp>max_speed) & samecamera_temp\n",
    "                    tokeep_sel = np.logical_not(np.concatenate(([True if error_frames[0]==True else False],error_frames)))\n",
    "                    numerrorframes = np.sum(np.logical_not(tokeep_sel))  # do this instead of summing error_frames, to keep the contingency for the 1st frame\n",
    "                    numobs = np.sum(tokeep_sel)\n",
    "                    camera,x,y,theta,ids,times,framenums = camera[tokeep_sel],x[tokeep_sel],y[tokeep_sel],theta[tokeep_sel],ids[tokeep_sel],times[tokeep_sel],framenums[tokeep_sel]\n",
    "                # account for the 2019 \"time shift\" of database queries.  The 'time' in the df is UTC time, but queries are assumed in UTC+2.\n",
    "                # So, if query 1am, it will return 23:00 the previous day.\n",
    "                # Account for this by shifting the framenumbers up two hours, and then using mod for wrap around\n",
    "                # With this, then the query returning 23:00 the previous day, has a framenumber for 01:00 the current day, which is correct\n",
    "                # and 22:00 the current day gets shifted to 24:00\n",
    "                if year==2019:\n",
    "                    framenums = np.mod(framenums + framesperhour*2,framesperhour*24)\n",
    "                # use this to select framenumbers if saving skipped trajectory versions\n",
    "                framenumsel = np.tile(False,len(framenums))\n",
    "                framenumsel[np.mod(framenums,saveskip)==0] = True\n",
    "                dftemp = dfblank.copy()\n",
    "                dftemp['framenum'] = framenums[framenumsel]\n",
    "                dftemp['x'] = x[framenumsel]\n",
    "                dftemp['y'] = y[framenumsel]\n",
    "                dftemp['theta'] = theta[framenumsel]\n",
    "                dftemp['camera'] = camera[framenumsel]\n",
    "                dftemp['uid'] = bee_uid\n",
    "                dftemp['daynum'] = daynum                    \n",
    "                dftemps.append(dftemp)\n",
    "    \n",
    "    dfcat = pd.concat(dftemps)\n",
    "    dfcat.reset_index(inplace=True)\n",
    "    dfcat.drop(columns=['index'],inplace=True)\n",
    "\n",
    "\n",
    "    print('writing to file....')\n",
    "    # output the results for this day as a pkl file\n",
    "    # savefile = resultsdir+'beetrajectories_skip'+str(saveskip)+'_'+str(daynum).zfill(3)+'.pklz'\n",
    "    # pickle.dump(dfcat, gzip.open(savefile,'wb'),protocol=4)\n",
    "    \n",
    "    ### updated:  output directly as an HDF file\n",
    "    columns = ['daynum', 'framenum', 'uid', 'x', 'y', 'camera']\n",
    "    dfcat.loc[:,columns] = df[columns].applymap(int)\n",
    "    outfile = resultsdir+'beetrajectories_'+str(daynum).zfill(3)+'.hdf'\n",
    "    dfcat.to_hdf(outfile,complevel=9,key='df', mode='w')  \n",
    "\n",
    "    print('wrote to:', savefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "daystoprocess= np.arange(0,bd.numdays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day:  2019-08-16 00:00:00\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "writing to file....\n"
     ]
    }
   ],
   "source": [
    "# parallel \n",
    "# pool = multiprocessing.Pool(processes=2)\n",
    "# pool.map(processday_dftraj,daystoprocess)\n",
    "# serial\n",
    "for daynum in daystoprocess:\n",
    "    processday_dftraj(daynum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
